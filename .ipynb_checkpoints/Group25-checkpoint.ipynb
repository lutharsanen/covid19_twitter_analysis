{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1> Big-Data Analytics </h1>\n",
    " <h4> Group 25 </h4>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project is to process a raw dataset from beginning to end, going through each of the steps in the data science pipeline. The ongoing Coronavirus crisis has upended not only our semester, but our lives, so our group could not think of a more fitting topic to examine with this project. Amidst the rapidly evolving threat, the most up-to-date sources have often been social media platforms online both for front-line ER doctors sharing clinical procedures (https://www.nytimes.com/2020/03/18/well/live/coronavirus-doctors-facebook-twitter-social-media-covid.html) and for the broader scientific community and reporters, sharing insights and ideas to combat the crisis. \n",
    "\n",
    "Its timeliness and reputation, as well as the company's relative openness to sharing data with developers via their API, made Twitter a logical source of data for this project.\n",
    "\n",
    "Early in the crisis, many people (though populists most prominently, with their anti-expertise bias) the world over claimed \"the media\" was overreacting to the crisis, and sensationalizing the threat. While this interpretation has been clearly born out as false, our group is interested in how closely the \"buzz\" around the virus correlates with the actual rates of infection. Much has been made about the virus's exponential spread: do posts about the virus in affected regions follow the same pattern? How can we understand not only the patterns within countries but the differences between countries?\n",
    "\n",
    "The deliverable of the project consists of a Jupyter notebook that includes the\n",
    "code used for exploring, processing, analyzing, and visualizing the data set and\n",
    "the data set itself (this can be download link). If the data set is very large, a\n",
    "sample of the data set can also be appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data collection/acquisition </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling the 30 day data (with & without geolocation data) from Twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data preprocessing/cleaning </h1>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts by region\n",
    "#plotting day-by-day bar charts counting mentions?\n",
    "#plotting on map of Italy with dot size showing the number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for tweets per day\n",
    "italia = {}\n",
    "ny = {}\n",
    "\n",
    "\n",
    "# array with name of all files in a folder\n",
    "italia_json_files = []\n",
    "ny_json_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_filesname(files, state):\n",
    "    if state == \"italy\":\n",
    "        for filenames in walk(r\"C:\\Users\\Lutharsanen\\Documents\\GitHub\\covi19_twitter_analysis\\twitterapi\\IT\"):\n",
    "            files.append(filenames)\n",
    "    elif state == \"ny\":\n",
    "        for filenames in walk(r\"C:\\Users\\Lutharsanen\\Documents\\GitHub\\covi19_twitter_analysis\\twitterapi\\NY\"):\n",
    "            files.append(filenames)\n",
    "    return files\n",
    "    \n",
    "def date_collector(data_set, d):\n",
    "    for i in data_set['results']:\n",
    "        if not date_transformer(i['created_at'][4:10]) in list(d.keys()):\n",
    "            d[date_transformer(i['created_at'][4:10])] = 1\n",
    "        else:\n",
    "            d[date_transformer(i['created_at'][4:10])] +=1\n",
    "\n",
    "    return d\n",
    "\n",
    "def files_walker(files, d, state):\n",
    "    if state == \"ny\":\n",
    "        for i in range(len(files[0][2])):\n",
    "            with open('twitterapi/NY/'+files[0][2][i], 'r', encoding='utf8', errors='ignore') as f:\n",
    "                region = json.load(f)\n",
    "                date_collector(region, d)\n",
    "    elif state ==\"italy\":\n",
    "        for i in range(len(files[0][2])):\n",
    "            with open('twitterapi/IT/'+files[0][2][i], 'r', encoding='utf8', errors='ignore') as f:\n",
    "                region = json.load(f)\n",
    "                date_collector(region, d)\n",
    "    return d\n",
    "\n",
    "def date_transformer(string_data):\n",
    "    date = string_data + ' 2020'\n",
    "    return datetime.strptime(date,\"%b %d %Y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Italy </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "date_collector() missing 1 required positional argument: 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f9a9881699a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mitalia_json_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_filesname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitalia_json_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'italy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mitalia_json_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles_walker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitalia_json_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitalia\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"italy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_italy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitalia\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_italy_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_italy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-46cbfd4eb830>\u001b[0m in \u001b[0;36mfiles_walker\u001b[1;34m(files, d, state)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'twitterapi/IT/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mregion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mdate_collector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: date_collector() missing 1 required positional argument: 'state'"
     ]
    }
   ],
   "source": [
    "italia_json_files = get_all_filesname(italia_json_files, 'italy')\n",
    "italia_json_files = files_walker(italia_json_files, italia, \"italy\")\n",
    "df_italy = pd.DataFrame.from_dict(italia, orient= 'index')\n",
    "df_italy_sorted = df_italy.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> New York </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_json_files = get_all_filesname(ny_json_files, 'ny')\n",
    "ny_json_files = files_walker(ny_json_files, ny,'ny')\n",
    "df_ny = pd.DataFrame.from_dict(ny, orient= 'index')\n",
    "df_ny_sorted = df_ny.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(len(df_italy_sorted[0]))]\n",
    "plt.plot(x,df_italy_sorted[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(len(df_ny_sorted[0]))]\n",
    "plt.plot(x,df_ny_sorted[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model/algorithm building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Luthy & @Joel: any good ideas for modeling this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data visualization & interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how the rates correlate when we control for twitter DAU  & MAU (daily/monthly active users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
